{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5da1cd8",
   "metadata": {},
   "source": [
    "\n",
    "# Training – Baseline PhaseDiff Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 00_shared_utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b06f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "# --- Core setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(7)\n",
    "\n",
    "n_fft = 512\n",
    "hop = 80\n",
    "win = n_fft\n",
    "\n",
    "MF_WIN = 3\n",
    "TB_WIN = 2\n",
    "mag_thresh_db = -60  # silence filtering threshold\n",
    "\n",
    "cfg = STFTCfg(sr=16000, n_fft=n_fft, hop=hop)\n",
    "stft_small = CausalSTFT(n_fft=n_fft, hop_length=hop, win_length=win).to(device)\n",
    "\n",
    "stft = CausalSTFT(cfg.n_fft, cfg.hop, cfg.n_fft).to(device)\n",
    "print(\"Training baseline model with\", cfg.__dict__)\n",
    "\n",
    "subset_paths = load_subset_paths(\"subset_paths.txt\")\n",
    "dl = make_dataloader(subset_paths, sr=cfg.sr, seconds=3, batch_size=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ff126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset ---\n",
    "# Reuse existing DataLoader if already in memory, otherwise we have to create one quickly\n",
    "if \"dl\" in globals():\n",
    "    print(\"Using existing dataloader (len:\", len(dl), \")\")\n",
    "else:\n",
    "    assert \"subset_paths\" in globals(), \"Need subset_paths or dl from previous session.\"\n",
    "    class ListAudioDataset(Dataset):\n",
    "        def __init__(self, paths, sr=16000, seconds=3):\n",
    "            self.paths = paths; self.sr = sr; self.samples = int(seconds * sr)\n",
    "        def __len__(self): return len(self.paths)\n",
    "        def __getitem__(self, idx):\n",
    "            p = self.paths[idx]\n",
    "            wav, sr = torchaudio.load(p)\n",
    "            wav = wav.mean(0, keepdim=True)\n",
    "            if sr != self.sr:\n",
    "                wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "            if wav.shape[-1] < self.samples:\n",
    "                reps = (self.samples + wav.shape[-1] - 1)//wav.shape[-1]\n",
    "                wav = wav.repeat(1, reps)\n",
    "            start = random.randint(0, wav.shape[-1]-self.samples)\n",
    "            return wav[:, start:start+self.samples].squeeze(0)\n",
    "    ds = ListAudioDataset(subset_paths, sr=cfg.sr, seconds=3)\n",
    "    dl = DataLoader(ds, batch_size=6, shuffle=True, drop_last=True, num_workers=0)\n",
    "    print(\"Created new dataloader with\", len(dl), \"batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e62983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model & Optimizer ---\n",
    "model = PhaseDiffPredictionModel(n_fft=cfg.n_fft, hop_length=cfg.hop, use_film=False).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vm(a,b): return -(torch.cos(a-b)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_fpd = total_bpd = 0.0\n",
    "    steps = 0\n",
    "    for wav_cpu in dl:\n",
    "        wav = wav_cpu.to(device)\n",
    "        with torch.no_grad():\n",
    "            mag, fpd_tgt, bpd_tgt = compute_mag_fpd_bpd(\n",
    "                wav, cfg.n_fft, cfg.hop, stft\n",
    "            )  # mag: (B, F, T)\n",
    "\n",
    "        # local inpainting 50% of the time\n",
    "        mag_in = mag\n",
    "        if random.random() < 0.5 and mag.shape[-1] > 3:\n",
    "            k = 2 if random.random() < 0.5 else 1  # k = 1 or 2 with equal prob\n",
    "            mag_in = inpaint_k_between_pairs_linear(mag, k=k)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        fpd_pred, bpd_pred, _ = model(mag=mag_in)\n",
    "\n",
    "        \n",
    "        loss_f = vm(fpd_tgt, fpd_pred)\n",
    "        loss_b = vm(bpd_tgt, bpd_pred)\n",
    "        loss = loss_f + loss_b\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        total_fpd += float(loss_f.detach())\n",
    "        total_bpd += float(loss_b.detach())\n",
    "        steps += 1\n",
    "\n",
    "    print(f\"[Epoch {ep:02d}] FPD {total_fpd/steps:+.4f} | BPD {total_bpd/steps:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save checkpoint ---\n",
    "save_ckpt(model, \"checkpoints/baseline_og.pth\", cfg)\n",
    "print(\"✅ Baseline model saved to checkpoints/baseline (+.json)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
