{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8448eabd",
   "metadata": {},
   "source": [
    "## my Imports, utils, and baseline checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471444f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio  \n",
    "\n",
    "\n",
    "%run 00_shared_utils.ipynb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- Load baseline model as TEACHER ------------------------------------------\n",
    "\n",
    "\n",
    "BASELINE_CKPT = \"checkpoints/baseline.pth\"\n",
    "\n",
    "baseline_model, cfg = load_ckpt(BASELINE_CKPT, device=device, use_film=False)\n",
    "baseline_model.eval()\n",
    "for p in baseline_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"Loaded baseline teacher from:\", BASELINE_CKPT)\n",
    "print(\"Baseline cfg:\", cfg.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751e3cd",
   "metadata": {},
   "source": [
    "## Positional encoding + StretchAware v2 mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dad880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sinusoidal positional encoding (time axis) ------------------------\n",
    "\n",
    "def sinusoidal_pe(T: int, dim: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Standard Transformer-style sinusoidal positional encoding.\n",
    "    Returns (dim, T).\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(dim, T, device=device)\n",
    "    position = torch.arange(T, device=device, dtype=torch.float32).unsqueeze(1)  # (T,1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, dim, 2, device=device, dtype=torch.float32)\n",
    "        * (-math.log(10000.0) / dim)\n",
    "    )\n",
    "    pe[0::2, :] = torch.sin(position * div_term).transpose(0, 1)\n",
    "    pe[1::2, :] = torch.cos(position * div_term).transpose(0, 1)\n",
    "    return pe  # (dim, T)\n",
    "\n",
    "\n",
    "# --- StretchAwarePhaseDiff v2 (teacher-student) ------------------------------\n",
    "\n",
    "class StretchAwarePhaseDiffV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Stretch-aware phase-difference predictor trained to mimic the baseline (teacher)\n",
    "    on spectrograms warped by a stretch factor z.\n",
    "\n",
    "    Input:\n",
    "        mag_z : (B, F, T)   -- warped linear magnitude (synthetic stretch)\n",
    "        z     : (B,) or (B,1) -- stretch factor per sample\n",
    "\n",
    "    Extra features:\n",
    "        - sinusoidal time positional encoding (non-learnable)\n",
    "        - log(z) as a global channel\n",
    "\n",
    "    Output:\n",
    "        - fpd_pred: (B, F-1, T)\n",
    "        - bpd_pred: (B, F,   T)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        pe_dim: int = 16,\n",
    "        hidden_channels: int = 64,\n",
    "        num_layers: int = 6,\n",
    "        k_f: int = 3,\n",
    "        k_t: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.pe_dim = pe_dim\n",
    "\n",
    "        # logmag (1) + PE (pe_dim) + z-channel (1)\n",
    "        in_channels = 1 + pe_dim + 1\n",
    "\n",
    "        self.in_proj = nn.Conv2d(in_channels, hidden_channels, kernel_size=1)\n",
    "\n",
    "        # Non-causal temporal conv stack with dilations to increase receptive field.\n",
    "        blocks = []\n",
    "        dilations = [1, 2, 4, 8, 16, 1][:num_layers]\n",
    "        for d in dilations:\n",
    "            blocks.append(\n",
    "                nn.Conv2d(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size=(k_f, k_t),\n",
    "                    padding=(k_f // 2, d * (k_t // 2)),   # symmetric padding in time\n",
    "                    dilation=(1, d),\n",
    "                )\n",
    "            )\n",
    "            blocks.append(nn.GELU())\n",
    "            blocks.append(nn.BatchNorm2d(hidden_channels))\n",
    "        self.net = nn.Sequential(*blocks)\n",
    "\n",
    "        # Separate heads for BPD and FPD (1 channel each)\n",
    "        self.out_bpd = nn.Conv2d(hidden_channels, 1, kernel_size=1)\n",
    "        self.out_fpd = nn.Conv2d(hidden_channels, 1, kernel_size=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, mag: torch.Tensor, z: torch.Tensor):\n",
    "        \"\"\"\n",
    "        mag : (B, F, T)   linear magnitude (warped by z)\n",
    "        z   : (B,) or (B,1) stretch factors\n",
    "\n",
    "        Returns:\n",
    "            fpd_pred: (B, F-1, T)\n",
    "            bpd_pred: (B, F,   T)\n",
    "        \"\"\"\n",
    "        B, Freq, T = mag.shape\n",
    "        device = mag.device\n",
    "\n",
    "        # 1) log-magnitude base channel\n",
    "        logmag = mag.clamp_min(1e-6).log10()\n",
    "        x = logmag.unsqueeze(1)  # (B,1,F,T)\n",
    "\n",
    "        # 2) sinusoidal positional encoding over time\n",
    "        pe = sinusoidal_pe(T, self.pe_dim, device=device)          # (pe_dim, T)\n",
    "        pe = pe.unsqueeze(0).unsqueeze(2).expand(B, self.pe_dim, Freq, T)  # (B,pe_dim,F,T)\n",
    "\n",
    "        # 3) stretch factor channel: use log(z) broadcast over F,T\n",
    "        if z.dim() == 1:\n",
    "            z = z.unsqueeze(1)  # (B,1)\n",
    "        z_norm = torch.log(z + 1e-6)  # (B,1)\n",
    "        z_img = z_norm.view(B, 1, 1, 1).expand(B, 1, Freq, T)      # (B,1,F,T)\n",
    "\n",
    "        feats = torch.cat([x, pe, z_img], dim=1)  # (B,1+pe_dim+1,F,T)\n",
    "\n",
    "        # 4) Conv trunk\n",
    "        h = self.in_proj(feats)\n",
    "        h = self.net(h)\n",
    "\n",
    "        # 5) Heads\n",
    "        bpd = self.out_bpd(h).squeeze(1)       # (B,F,T)\n",
    "        fpd_full = self.out_fpd(h).squeeze(1)  # (B,F,T)\n",
    "        fpd = fpd_full[:, :-1, :]              # (B,F-1,T) â€“ match teacher target shape\n",
    "\n",
    "        return fpd, bpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53934118",
   "metadata": {},
   "source": [
    "## Config, data, model, stretch factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb845acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core setup ---------------------------------------------------------------\n",
    "\n",
    "set_seed(7)\n",
    "\n",
    "# I use cfg loaded from baseline ckpt for consistency\n",
    "sr = cfg.sr\n",
    "n_fft = cfg.n_fft\n",
    "hop = cfg.hop\n",
    "\n",
    "stft = CausalSTFT(n_fft, hop, n_fft).to(device)\n",
    "print(\"Training StretchAwarePhaseDiff v2 (teacher) with cfg:\", cfg.__dict__)\n",
    "\n",
    "\n",
    "subset_paths = load_subset_paths(\"subset_paths.txt\")\n",
    "print(\"Total files in subset_paths:\", len(subset_paths))\n",
    "\n",
    "\n",
    "# N_FILES = 200   # for testing pruproses before training big model\n",
    "# subset_paths = subset_paths[:N_FILES]\n",
    "# print(\"Using first\", len(subset_paths), \"files for v2 training\")\n",
    "\n",
    "dl = make_dataloader(\n",
    "    subset_paths,\n",
    "    sr=sr,\n",
    "    seconds=3.0,    # shorter clips to speed up\n",
    "    batch_size=6,   # we can also adjust batch here for increased speed\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Student model\n",
    "student = StretchAwarePhaseDiffV2(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop,\n",
    "    pe_dim=16,\n",
    "    hidden_channels=64,\n",
    "    num_layers=6,\n",
    "    k_f=3,\n",
    "    k_t=5,\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "epochs = 8   \n",
    "\n",
    "n_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Student model parameters: {n_params/1e6:.2f} M\")\n",
    "\n",
    "# Stretch factors to sample \n",
    "stretch_factors = [1.0, 1.5, 2.0]  \n",
    "print(\"Stretch factors:\", stretch_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8088a",
   "metadata": {},
   "source": [
    "## Teacher-student training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training loop: student mimics baseline on stretched spectrograms -----------\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ckpt_path = \"checkpoints/stretchaware_teacher.pth\"\n",
    "\n",
    "def vm_loss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return -(torch.cos(a - b)).mean()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    student.train()\n",
    "    total_fpd = 0.0\n",
    "    total_bpd = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch_idx, wav_cpu in enumerate(dl):\n",
    "        wav = wav_cpu.to(device)  # (B, N)\n",
    "        B, N = wav.shape\n",
    "\n",
    "        # 1) compute magnitude of original audio\n",
    "        with torch.no_grad():\n",
    "            mag_orig, _, _ = compute_mag_fpd_bpd(wav, n_fft, hop, stft)  # (B,F,T)\n",
    "\n",
    "        # 2) sample a stretch factor z for this batch\n",
    "        z_val = random.choice(stretch_factors)\n",
    "        z_batch = torch.full((B,), float(z_val), device=device)  # (B,)\n",
    "\n",
    "        # 3) warp magnitude along time to simulate stretch\n",
    "        mag_z = logmag_stretch_then_match_T(mag_orig, z_val)  # (B,F,T)\n",
    "\n",
    "        # 4) baseline TEACHER: get FPD/BPD targets on warped magnitude\n",
    "        with torch.no_grad():\n",
    "            out_teacher = baseline_model(mag=mag_z)\n",
    "            # baseline_model forward returns (fpd_pred, bpd_pred, aux) in setup\n",
    "            if isinstance(out_teacher, (tuple, list)):\n",
    "                fpd_teacher, bpd_teacher = out_teacher[0], out_teacher[1]\n",
    "            else:\n",
    "                raise RuntimeError(\"Unexpected baseline_model output format\")\n",
    "\n",
    "        # 5) STUDENT forward on same mag_z + explicit z\n",
    "        fpd_student, bpd_student = student(mag_z, z_batch)\n",
    "\n",
    "        # 6) losses (teacher vs student)\n",
    "        loss_f = vm_loss(fpd_teacher, fpd_student)\n",
    "        loss_b = vm_loss(bpd_teacher, bpd_student)\n",
    "        loss = loss_f + loss_b\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_fpd += loss_f.item()\n",
    "        total_bpd += loss_b.item()\n",
    "        steps += 1\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  [ep {ep}] batch {batch_idx}, loss_f={loss_f.item():.4f}, loss_b={loss_b.item():.4f}, z={z_val}\")\n",
    "\n",
    "    mean_fpd = total_fpd / steps\n",
    "    mean_bpd = total_bpd / steps\n",
    "    print(f\"[Epoch {ep}/{epochs}] mean loss_f={mean_fpd:.4f}, mean loss_b={mean_bpd:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"state_dict\": student.state_dict(),\n",
    "        \"cfg\": cfg.__dict__,\n",
    "        \"stretch_factors\": stretch_factors,\n",
    "        \"note\": \"StretchAwarePhaseDiff v2 (teacher-student on warped mags)\",\n",
    "    }, ckpt_path)\n",
    "\n",
    "print(\"Training done. Final checkpoint saved to\", ckpt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
